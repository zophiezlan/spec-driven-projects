# Impact Measurement Framework

**Program Name**: [Name]  
**Evaluation Period**: [Start Date] to [End Date]  
**Framework Version**: [Number]  
**Last Updated**: [Date]

---

## Purpose of This Framework

This framework guides how we measure and demonstrate the **difference** our program makes. It ensures:

1. **Accountability**: We track what we promised to deliver
2. **Learning**: We understand what works and what doesn't
3. **Improvement**: We adapt based on evidence
4. **Communication**: We can show impact to funders, community, and stakeholders

**Evaluation Principles**:

- **Participatory**: Participants co-design evaluation and interpret findings
- **Culturally safe**: Methods respect and reflect community values
- **Practical**: Designed to be feasible with available resources
- **Ethical**: Informed consent, confidentiality, do no harm

---

## Evaluation Questions

### Process Questions (Are we doing what we planned?)

1. Is the program implemented as designed?
2. Are we reaching our target population?
3. Are participants satisfied with the program?
4. Are partnerships working effectively?
5. Are we operating within budget?

### Outcome Questions (What changed for participants?)

1. Did participants' knowledge/skills/attitudes change?
2. Did participants' wellbeing improve?
3. Did participants access services/supports they needed?
4. Did participants feel empowered and connected?

### Impact Questions (What broader change occurred?)

1. Did community attitudes toward [issue] shift?
2. Did services become more accessible or responsive?
3. Did policy or practice change?

### Equity Questions (Who benefited and who didn't?)

1. Who participated and who was missed?
2. Did outcomes differ by subgroup?
3. Were barriers reduced for marginalized groups?

---

## Indicators Framework

### Process Indicators

| Indicator              | Definition                      | Target          | Data Source        | Frequency      |
| ---------------------- | ------------------------------- | --------------- | ------------------ | -------------- |
| **Fidelity to design** | Activities delivered as planned | 90%+ adherence  | Program logs       | Weekly         |
| **Reach**              | Number of people engaged        | [Target number] | Attendance records | Weekly         |
| **Retention**          | % completing program            | 75%+            | Attendance records | End of program |
| **Satisfaction**       | % reporting positive experience | 85%+            | Exit survey        | End of program |
| **Partnerships**       | Number of active partnerships   | [Target number] | MOU tracking       | Quarterly      |
| **Budget**             | Spending vs. plan               | Within 5%       | Finance reports    | Monthly        |

---

### Output Indicators

| Indicator                 | Definition                      | Target     | Data Source      | Frequency |
| ------------------------- | ------------------------------- | ---------- | ---------------- | --------- |
| **Sessions delivered**    | Number of sessions held         | [Target]   | Program logs     | Weekly    |
| **Participants engaged**  | Unique individuals involved     | [Target]   | Sign-in sheets   | Weekly    |
| **Peer hours**            | Hours of peer worker engagement | [Target]   | Timesheets       | Monthly   |
| **Resources distributed** | Materials/supplies given        | [Target]   | Distribution log | Weekly    |
| **Referrals made**        | Connections to services         | [Target]   | Referral forms   | Weekly    |
| **Referrals completed**   | Successful connections          | [Target %] | Follow-up calls  | Monthly   |

---

### Outcome Indicators (Short-Term: 0-3 months)

| Indicator          | Definition                       | Measurement Tool | Target           | When           |
| ------------------ | -------------------------------- | ---------------- | ---------------- | -------------- |
| **Knowledge gain** | Pre/post knowledge score         | Knowledge quiz   | 30% increase     | Baseline & end |
| **Confidence**     | Self-rated confidence in [skill] | 5-point scale    | 1 point increase | Baseline & end |
| **Awareness**      | Knowledge of available services  | Survey question  | 80% aware        | Baseline & end |
| **Stigma (self)**  | Internalized stigma score        | Validated scale  | 20% decrease     | Baseline & end |
| **Connection**     | Number of peer connections       | Survey question  | 3+ connections   | End            |
| **Satisfaction**   | Overall satisfaction rating      | 5-point scale    | 4+ average       | End            |

---

### Outcome Indicators (Medium-Term: 3-12 months)

| Indicator           | Definition                      | Measurement Tool   | Target          | When               |
| ------------------- | ------------------------------- | ------------------ | --------------- | ------------------ |
| **Wellbeing**       | Psychological distress (K10)    | K10 scale          | 10% improvement | Baseline, 3mo, 6mo |
| **Service access**  | Using [X] services regularly    | Survey question    | 60% accessing   | 3mo, 6mo           |
| **Behavior change** | Adoption of [practice]          | Self-report survey | 50% adopting    | 3mo, 6mo           |
| **Peer support**    | Giving/receiving peer support   | Survey question    | 70% engaged     | 3mo, 6mo           |
| **Empowerment**     | Agency and control score        | Empowerment scale  | 15% increase    | Baseline, 6mo      |
| **Participation**   | Attending other NUAA activities | Admin records      | 40% participate | 6mo                |

---

### Outcome Indicators (Long-Term: 12+ months)

| Indicator            | Definition                    | Measurement Tool | Target          | When           |
| -------------------- | ----------------------------- | ---------------- | --------------- | -------------- |
| **Health outcomes**  | Self-rated health status      | Survey question  | 20% improvement | Baseline, 12mo |
| **Harm reduction**   | [Specific harm indicator]     | Self-report      | [Target %]      | Baseline, 12mo |
| **Social inclusion** | Social connections score      | Inclusion scale  | 15% increase    | Baseline, 12mo |
| **Leadership**       | Taking on peer/advocacy roles | Tracking data    | 10% of cohort   | 12mo           |
| **Sustained change** | Maintained outcomes from 6mo  | Repeat survey    | 70% sustained   | 12mo           |

---

### Impact Indicators (Community/System Level)

| Indicator                  | Definition                                | Measurement Tool  | Target           | When          |
| -------------------------- | ----------------------------------------- | ----------------- | ---------------- | ------------- |
| **Community stigma**       | Attitudes toward [group]                  | Community survey  | 10% improvement  | Baseline, end |
| **Service responsiveness** | Partner services report improved practice | Partner survey    | 5+ services      | End           |
| **Policy influence**       | Citations in policy documents             | Document analysis | 3+ citations     | Ongoing       |
| **Sector reach**           | Presentations/publications                | Tracking log      | 5+ dissemination | End           |
| **Sustainability**         | Funding secured for continuation          | Grant tracking    | [Target $]       | End           |

---

## Data Collection Methods

### Quantitative Methods

#### 1. Surveys

**Pre-Program Survey** (Baseline):

- Demographics (age, gender, location, etc.)
- Knowledge questions
- Validated scales (K10, empowerment, stigma)
- Service access history

**Post-Program Survey** (Immediately after):

- Knowledge questions (same as pre)
- Validated scales (same as pre)
- Satisfaction questions
- Open-ended feedback

**Follow-Up Surveys** (3, 6, 12 months):

- Validated scales (repeated)
- Service access update
- Behavior change questions
- Open-ended reflection

**Survey Design Principles**:

- Plain language, short (10-15 min max)
- Available in multiple formats (online, paper, phone)
- Incentivized (gift voucher, entry to prize draw)
- Culturally appropriate imagery and language

#### 2. Program Monitoring Data

- Attendance records (who attended each session)
- Participation observations (who contributed, group dynamics)
- Referral tracking (who referred, outcome)
- Resource distribution logs

#### 3. Administrative Data

- Service usage data (with consent from partners)
- Health data (if available and consented)
- Employment/housing status (if relevant and consented)

---

### Qualitative Methods

#### 1. In-Depth Interviews

**Who**: Purposive sample of 10-15 participants  
**When**: Mid-program and end  
**Questions**:

- Tell me about your experience in this program...
- What has changed for you?
- What was most helpful? What could be improved?
- How has this affected your life?

**Method**:

- Semi-structured interview guide
- 30-60 minutes
- Audio recorded (with consent) or detailed notes
- Participant chooses location (in-person, phone, video)
- Honorarium: $50 gift voucher

#### 2. Focus Groups

**Who**: Groups of 6-8 participants  
**When**: Mid-program for feedback, end for reflection  
**Topics**:

- Program experience and satisfaction
- Changes observed (in self and peers)
- Recommendations for improvement
- Future directions

**Method**:

- 60-90 minutes
- Facilitated by peer researcher + note-taker
- Catering provided
- Honorarium: $50 gift voucher per person

#### 3. Case Studies

**Who**: 3-5 participants willing to share their story in-depth  
**Method**:

- Multiple interviews over time
- Rich narrative of their journey
- Photo voice or other creative methods (optional)
- Co-created with participant (they control what's shared)

**Use**: For reports, publications, funding proposals (with consent)

#### 4. Observation & Reflective Practice

**Who**: Program staff and peer workers  
**Method**:

- Session reflection notes (what worked, what didn't)
- Critical incidents (significant moments)
- Team debriefs

**Purpose**: Capture nuances, adapt in real-time, professional development

---

### Participatory Methods

#### 1. Most Significant Change (MSC)

**Process**:

- Participants share stories of change
- Group discusses and selects most significant stories
- Stories analyzed for themes

**Why**: Centers participant voice, captures unexpected outcomes

#### 2. Outcome Harvesting

**Process**:

- Participants identify changes they've observed (in self, others, community)
- Changes are documented and verified
- Changes are analyzed for contribution (not attribution)

**Why**: Flexible, useful for complex programs with multiple influences

#### 3. Participatory Data Analysis

**Process**:

- Participants involved in interpreting survey results
- Participants co-create recommendations
- Participants review draft reports

**Why**: Ensures findings are accurate and culturally meaningful

---

## Data Collection Timeline

| Timepoint                  | Data Collected                             | Method                          | Responsible     |
| -------------------------- | ------------------------------------------ | ------------------------------- | --------------- |
| **Week 1 (Baseline)**      | Demographics, pre-survey, consent          | Survey, intake form             | Program staff   |
| **Weeks 2-4**              | Attendance, observations                   | Program logs                    | Program staff   |
| **Week 4-6 (Mid-program)** | Feedback, mid-point interviews             | Survey, interviews              | Peer researcher |
| **Week 8 (End)**           | Post-survey, exit interviews, focus groups | Survey, interviews, FG          | Peer researcher |
| **Month 3**                | Follow-up survey                           | Online/phone survey             | Program staff   |
| **Month 6**                | Follow-up survey, case study interviews    | Online/phone survey, interviews | Peer researcher |
| **Month 12**               | Final follow-up survey                     | Online/phone survey             | Program staff   |
| **Throughout**             | Reflective notes, critical incidents       | Staff notes                     | All staff       |

---

## Data Management & Ethics

### Informed Consent

- **When**: Before any data collection
- **Process**: Written consent form explained verbally, opportunity to ask questions
- **Content**: What data collected, how used, who has access, how privacy protected, right to withdraw
- **Special considerations**: Separate consent for interviews, case studies, photo/video

### Confidentiality

- **De-identification**: All data coded (no names in datasets)
- **Secure storage**: Encrypted digital files, locked physical files
- **Access**: Only evaluation team (staff + peer researchers)
- **Reporting**: Aggregated data only, no identifying details

### Data Sharing

- **With funders**: Aggregated results only (as per contract)
- **With partners**: Only with explicit participant consent
- **With participants**: Participants can access their own data
- **With community**: De-identified findings in plain language

### Aboriginal & Torres Strait Islander Data

- **Sovereignty**: Aboriginal & Torres Strait Islander participants control their data
- **Consultation**: Aboriginal & Torres Strait Islander researchers/advisors guide process
- **Benefits**: Data used to benefit community

### Vulnerable Participants

- **Capacity**: Assess capacity to consent
- **Coercion**: Ensure participation is voluntary (no penalty for non-participation)
- **Harm minimization**: Trauma-informed questions, support available
- **Mandatory reporting**: Clear about limits of confidentiality

---

## Data Analysis Plan

### Quantitative Analysis

**Descriptive Statistics**:

- Demographics (frequencies, percentages)
- Mean scores on scales (pre, post, follow-up)
- Attendance rates, completion rates

**Inferential Statistics**:

- Paired t-tests (pre/post change)
- ANOVA (differences by subgroup)
- Regression (predictors of outcomes)

**Software**: Excel, SPSS, R, or similar

**Who**: Program coordinator with evaluation expertise, or external evaluator

---

### Qualitative Analysis

**Thematic Analysis**:

1. **Familiarization**: Read/listen to all data
2. **Coding**: Identify meaningful segments, label with codes
3. **Theme development**: Group codes into themes
4. **Review**: Check themes against data
5. **Define**: Name and describe themes
6. **Report**: Write up findings with quotes

**Software**: NVivo, Dedoose, or manual (Word, spreadsheets)

**Who**: Peer researcher + program staff (collaborative analysis)

---

### Mixed Methods Integration

**Triangulation**:

- Compare quantitative findings with qualitative themes
- Do they confirm, expand, or contradict each other?

**Explanation**:

- Use qualitative data to explain quantitative results
- e.g., Why did wellbeing improve? (interviews reveal mechanisms)

**Expansion**:

- Use different methods for different questions
- e.g., Surveys show what changed, interviews show how and why

---

## Reporting & Dissemination

### Interim Report (Mid-Program)

**Audience**: Program team, management  
**Purpose**: Check progress, make adjustments  
**Content**:

- Recruitment and retention update
- Early feedback
- Budget tracking
- Recommendations for adaptation

**Format**: Brief report (5 pages), team meeting

---

### Final Report (End of Program)

**Audience**: Funder, management, board  
**Purpose**: Demonstrate outcomes, accountability, learning  
**Content**:

- Executive summary
- Process evaluation (what we did)
- Outcome evaluation (what changed)
- Impact evaluation (broader change)
- Lessons learned
- Recommendations
- Financial acquittal

**Format**: Formal report (20-30 pages), presentation

---

### Community Report (Plain Language)

**Audience**: Participants, community, partners  
**Purpose**: Share findings accessibly, build trust  
**Content**:

- What we did (simple language)
- What we found (visuals, quotes)
- What this means
- Thank you and next steps

**Format**: 2-page visual report, community forum

---

### Academic Dissemination

**Audience**: Harm reduction sector, researchers, policymakers  
**Purpose**: Contribute to evidence base, influence practice/policy  
**Formats**:

- Conference presentation
- Peer-reviewed publication
- Policy brief

**Authorship**: Peer researchers as co-authors

---

### Ongoing Communication

**During Program**:

- Updates in NUAA newsletter
- Social media posts (with consent)
- Funder check-ins (as required)

---

## Quality Assurance

### Validity (Are we measuring what we think we're measuring?)

- Use validated scales where possible
- Pilot test new questions
- Triangulate methods (surveys + interviews)

### Reliability (Are our measures consistent?)

- Standardized data collection procedures
- Training for all data collectors
- Inter-rater reliability checks (if multiple observers)

### Cultural Validity

- Peer review of questions
- Community advisory input
- Language and imagery appropriate

### Bias Minimization

- Peer researchers (not just "experts")
- Anonymous surveys
- Independent analysis (external perspective)

---

## Use of Evaluation Findings

### For Learning

- Team debrief: What worked? What didn't?
- Document lessons in accessible format
- Share with future cohorts of staff/peers

### For Improvement

- Adapt program in real-time (formative evaluation)
- Design next iteration based on findings

### For Accountability

- Show funders we delivered what we promised
- Demonstrate value for money

### For Advocacy

- Use findings to advocate for continued/increased funding
- Contribute to evidence base for harm reduction
- Challenge stigma with data

### For Empowerment

- Participants see their voices and experiences valued
- Participants can advocate for themselves using data

---

## Evaluation Budget

| Item                           | Cost          |
| ------------------------------ | ------------- |
| Survey software/tools          | $[Amount]     |
| Incentives (gift vouchers)     | $[Amount]     |
| Peer researcher time           | $[Amount]     |
| Staff evaluation time          | $[Amount]     |
| External evaluator (if needed) | $[Amount]     |
| Printing (surveys, reports)    | $[Amount]     |
| **Total**                      | **$[Amount]** |

**% of Total Program Budget**: [Typically 5-15%]

---

## Risks & Mitigation

| Risk                                    | Mitigation                                                                   |
| --------------------------------------- | ---------------------------------------------------------------------------- |
| Low survey response rates               | Incentives, multiple formats, short surveys, regular reminders               |
| Participant attrition (can't follow up) | Collect multiple contact methods, stay connected via social media/newsletter |
| Sensitive topics cause distress         | Trauma-informed questions, support available, right to skip questions        |
| Data lost or breached                   | Regular backups, encrypted storage, staff training                           |
| Findings are negative/disappointing     | Frame as learning, focus on what worked and what to improve                  |

---

## Evaluation Team

| Role                        | Responsibility                           | Name                 |
| --------------------------- | ---------------------------------------- | -------------------- |
| Evaluation Lead             | Overall design & reporting               | [Name, Role]         |
| Peer Researcher(s)          | Data collection, analysis, co-authorship | [Name(s)]            |
| Program Staff               | Monitoring data, support data collection | [Name(s)]            |
| External Advisor (optional) | Technical guidance                       | [Name, Organization] |

**Peer Researcher Role**:

- Bring lived experience perspective
- Build trust with participants
- Co-design data collection tools
- Conduct interviews/focus groups
- Co-analyze data
- Co-author reports
- Remuneration: $300/session or $[hourly rate]

---

## Review & Adaptation

**This evaluation framework is flexible.**

- Review after first month and adjust if needed
- Be responsive to participant feedback
- Add/remove methods based on feasibility

**Version History**:
| Version | Date | Changes | Updated By |
|---------|------|---------|------------|
| 1.0 | [Date] | Initial framework | [Name] |

---

## Resources & Support

**NUAA Evaluation Resources**:

- [Link to other NUAA evaluation reports]
- [Contact evaluation lead for support]

**External Resources**:

- Better Evaluation (bettervaluation.org)
- CDC Evaluation Framework
- UNSW Social Policy Research Centre
- Harm Reduction International resources

---

**This framework ensures we can demonstrate impact while honoring participant experiences and contributing to the evidence base for peer-led harm reduction.**
